"""
ìŠ¤ë§ˆíŠ¸ íŒ©í† ë¦¬ ì „ë ¥ ì˜ˆì¸¡ - ë°ì´í„° ì „ì²˜ë¦¬ ëª¨ë“ˆ
===================================================

ì‹œê³„ì—´ ë°ì´í„° ì „ì²˜ë¦¬ ì£¼ìš” ë‹¨ê³„:
1. ê²°ì¸¡ì¹˜(Missing Value) ì²˜ë¦¬
2. ë…¸ì´ì¦ˆ(Noise) ì œê±°  
3. ì´ìƒì¹˜(Outlier) ì²˜ë¦¬
4. ìŠ¤ì¼€ì¼ë§(Scaling)
5. ì—…ìƒ˜í”Œë§/ë‹¤ìš´ìƒ˜í”Œë§
6. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy import signal, stats
from scipy.interpolate import interp1d
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.impute import KNNImputer
import warnings
warnings.filterwarnings('ignore')

import os
import sys
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Optional, Union

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì„¤ì •
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

# í•œê¸€ í°íŠ¸ ì„¤ì •
plt.style.use('seaborn-v0_8')
plt.rcParams['figure.figsize'] = (15, 8)

# í•œê¸€ í°íŠ¸ ì„¤ì • (ìš´ì˜ì²´ì œë³„)
import platform
import matplotlib.font_manager as fm

def setup_korean_font():
    """í•œê¸€ í°íŠ¸ ì„¤ì •"""
    system = platform.system()
    
    if system == 'Darwin':  # macOS
        # macOSì—ì„œ ì‚¬ìš© ê°€ëŠ¥í•œ í•œê¸€ í°íŠ¸ë“¤
        korean_fonts = ['AppleGothic', 'Apple SD Gothic Neo', 'NanumGothic', 'NanumBarunGothic']
        
        for font in korean_fonts:
            try:
                plt.rcParams['font.family'] = font
                # í…ŒìŠ¤íŠ¸í•´ë³´ê¸°
                fig, ax = plt.subplots(figsize=(1, 1))
                ax.text(0.5, 0.5, 'í…ŒìŠ¤íŠ¸', fontsize=12)
                plt.close(fig)
                print(f"í•œê¸€ í°íŠ¸ ì„¤ì • ì™„ë£Œ: {font}")
                break
            except:
                continue
        else:
            print("í•œê¸€ í°íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            
    elif system == 'Windows':
        try:
            plt.rcParams['font.family'] = 'Malgun Gothic'
            print("í•œê¸€ í°íŠ¸ ì„¤ì • ì™„ë£Œ: Malgun Gothic")
        except:
            print("í•œê¸€ í°íŠ¸ ì„¤ì • ì‹¤íŒ¨. ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
            
    else:  # Linux
        try:
            plt.rcParams['font.family'] = 'NanumGothic'
            print("í•œê¸€ í°íŠ¸ ì„¤ì • ì™„ë£Œ: NanumGothic")
        except:
            print("í•œê¸€ í°íŠ¸ ì„¤ì • ì‹¤íŒ¨. ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    # ìŒìˆ˜ í‘œì‹œ ë¬¸ì œ í•´ê²°
    plt.rcParams['axes.unicode_minus'] = False

# í•œê¸€ í°íŠ¸ ì„¤ì • ì‹¤í–‰
setup_korean_font()

class PowerDataPreprocessor:
    """
    ìŠ¤ë§ˆíŠ¸ íŒ©í† ë¦¬ ì „ë ¥ ë°ì´í„° ì „ì²˜ë¦¬ í´ë˜ìŠ¤
    
    ì£¼ìš” ê¸°ëŠ¥:
    - ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (ë³´ê°„ë²•, ì‹œê³„ì—´ ë¶„í•´ ê¸°ë°˜)
    - ì´ìƒì¹˜ íƒì§€ ë° ì²˜ë¦¬ (IQR, 3-sigma, LOF)
    - ë…¸ì´ì¦ˆ í•„í„°ë§ (ì´ë™í‰ê· , ì¹¼ë§Œ í•„í„°)
    - ìŠ¤ì¼€ì¼ë§ (Standard, MinMax, Robust)
    - í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ (ì‹œê°„ íŠ¹ì„±, ì§€ì—° íŠ¹ì„±, í†µê³„ì  íŠ¹ì„±)
    """
    
    def __init__(self, config: Dict = None):
        """
        Args:
            config: ì „ì²˜ë¦¬ ì„¤ì • ë”•ì…”ë„ˆë¦¬
        """
        self.config = config or self._get_default_config()
        self.scalers = {}
        self.preprocessing_history = []
        self.feature_names = []
        
    def _get_default_config(self) -> Dict:
        """ê¸°ë³¸ ì „ì²˜ë¦¬ ì„¤ì •"""
        return {
            'missing_value': {
                'method': 'seasonal_decomposition',  # 'forward_fill', 'interpolate', 'seasonal_decomposition'
                'interpolation_method': 'linear',
                'seasonal_period': 24
            },
            'outlier_detection': {
                'method': 'iqr',  # 'iqr', 'zscore', 'modified_zscore', 'isolation_forest'
                'threshold': 1.5,  # IQRì˜ ê²½ìš° 1.5, z-scoreì˜ ê²½ìš° 3
                'action': 'cap'  # 'remove', 'cap', 'interpolate'
            },
            'noise_filtering': {
                'method': 'moving_average',  # 'moving_average', 'exponential_smoothing', 'kalman'
                'window_size': 3,
                'alpha': 0.3  # ì§€ìˆ˜í‰í™œ ê³„ìˆ˜
            },
            'scaling': {
                'method': 'standard',  # 'standard', 'minmax', 'robust'
                'feature_range': (0, 1)
            },
            'feature_engineering': {
                'time_features': True,
                'lag_features': True,
                'rolling_features': True,
                'lag_periods': [1, 6, 12, 24, 48],
                'rolling_windows': [6, 12, 24]
            }
        }
    
    def preprocess_data(self, data: pd.DataFrame, target_columns: List[str] = None) -> pd.DataFrame:
        """
        ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        
        Args:
            data: ì›ë³¸ ë°ì´í„°í”„ë ˆì„
            target_columns: ì „ì²˜ë¦¬í•  ì»¬ëŸ¼ ë¦¬ìŠ¤íŠ¸ (Noneì¸ ê²½ìš° ìˆ«ìí˜• ì»¬ëŸ¼ ëª¨ë‘)
            
        Returns:
            ì „ì²˜ë¦¬ëœ ë°ì´í„°í”„ë ˆì„
        """
        print("ğŸ”§ ë°ì´í„° ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        print("="*50)
        
        # ë°ì´í„° ë³µì‚¬
        processed_data = data.copy()
        
        # ì‹œê°„ ì»¬ëŸ¼ ì²˜ë¦¬
        processed_data = self._process_datetime_column(processed_data)
        
        # íƒ€ê²Ÿ ì»¬ëŸ¼ ì„¤ì •
        if target_columns is None:
            target_columns = processed_data.select_dtypes(include=[np.number]).columns.tolist()
            # ì‹œê°„ ê´€ë ¨ ì»¬ëŸ¼ ì œì™¸
            time_cols = ['year', 'month', 'day', 'hour', 'minute', 'weekday', 'quarter']
            target_columns = [col for col in target_columns if col not in time_cols]
        
        print(f"ğŸ“Š ì „ì²˜ë¦¬ ëŒ€ìƒ ì»¬ëŸ¼: {len(target_columns)}ê°œ")
        
        # 1. ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        processed_data = self._handle_missing_values(processed_data, target_columns)
        
        # 2. ì´ìƒì¹˜ ì²˜ë¦¬
        processed_data = self._handle_outliers(processed_data, target_columns)
        
        # 3. ë…¸ì´ì¦ˆ í•„í„°ë§
        processed_data = self._apply_noise_filtering(processed_data, target_columns)
        
        # 4. í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
        processed_data = self._feature_engineering(processed_data, target_columns)
        
        # 5. ìŠ¤ì¼€ì¼ë§ (ë§ˆì§€ë§‰ì— ìˆ˜í–‰)
        processed_data = self._apply_scaling(processed_data, target_columns)
        
        # ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½
        self._print_preprocessing_summary(data, processed_data)
        
        return processed_data
    
    def _process_datetime_column(self, data: pd.DataFrame) -> pd.DataFrame:
        """ì‹œê°„ ì»¬ëŸ¼ ì²˜ë¦¬ ë° ì‹œê°„ íŠ¹ì„± ì¶”ì¶œ"""
        
        # localtime ì»¬ëŸ¼ë§Œ ì‚¬ìš© (timestamp ë¬´ì‹œ)
        time_col = None
        if 'localtime' in data.columns:
            time_col = 'localtime'
        else:
            print("localtime ì»¬ëŸ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return data
        
        # localtime ì»¬ëŸ¼ ë³€í™˜ (YYYYMMDDHHMMSS í˜•íƒœ)
        print(f"localtime ì»¬ëŸ¼ ë³€í™˜ ì¤‘..")
        data[time_col] = pd.to_datetime(data[time_col].astype(str), format='%Y%m%d%H%M%S', errors='coerce')
        
        # ì˜ëª» ë³€í™˜ëœ ë‚ ì§œ ì²´í¬
        na_count = data[time_col].isna().sum()
        if na_count > 0:
            print(f"{na_count}ê°œì˜ localtime ê°’ì´ ì˜¬ë°”ë¥¸ ë‚ ì§œë¡œ ë³€í™˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # ì‹œê°„ ì •ë ¬
        data = data.sort_values(time_col).reset_index(drop=True)
        
        print(f"ì‹œê°„ ì»¬ëŸ¼ ì²˜ë¦¬ ì™„ë£Œ: {time_col}")
        print(f"   - ì‹œê°„ ë²”ìœ„: {data[time_col].min()} ~ {data[time_col].max()}")
        print(f"   - ì´ {len(data)}ê°œ ì‹œì ")
        
        return data
    
    def _handle_missing_values(self, data: pd.DataFrame, target_columns: List[str]) -> pd.DataFrame:
        """ê²°ì¸¡ì¹˜ ì²˜ë¦¬"""
        print("\nğŸ” ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì¤‘...")
        
        method = self.config['missing_value']['method']
        
        missing_before = data[target_columns].isnull().sum().sum()
        print(f"ì²˜ë¦¬ ì „ ê²°ì¸¡ì¹˜: {missing_before:,}ê°œ")
        
        if missing_before == 0:
            print("ê²°ì¸¡ì¹˜ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return data
        
        for column in target_columns:
            if data[column].isnull().sum() > 0:
                if method == 'forward_fill':
                    data[column] = data[column].fillna(method='ffill').fillna(method='bfill')
                
                elif method == 'interpolate':
                    interp_method = self.config['missing_value']['interpolation_method']
                    data[column] = data[column].interpolate(method=interp_method)
                
                elif method == 'seasonal_decomposition':
                    data[column] = self._seasonal_decomposition_imputation(data[column])
                
                else:
                    # ê¸°ë³¸ê°’: ì„ í˜• ë³´ê°„
                    data[column] = data[column].interpolate(method='linear')
        
        missing_after = data[target_columns].isnull().sum().sum()
        print(f"ì²˜ë¦¬ í›„ ê²°ì¸¡ì¹˜: {missing_after:,}ê°œ")
        print(f"ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì™„ë£Œ ({method} ë°©ë²• ì‚¬ìš©)")
        
        self.preprocessing_history.append(f"ê²°ì¸¡ì¹˜ ì²˜ë¦¬: {method}")
        
        return data
    
    def _seasonal_decomposition_imputation(self, series: pd.Series) -> pd.Series:
        """ì‹œê³„ì—´ ë¶„í•´ ê¸°ë°˜ ê²°ì¸¡ì¹˜ ë³´ê°„"""
        from statsmodels.tsa.seasonal import seasonal_decompose
        
        if series.isnull().all():
            return series
        
        # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ë¶„í•´ ìˆ˜í–‰
        non_null_count = series.count()
        period = self.config['missing_value']['seasonal_period']
        
        if non_null_count < period * 2:
            # ë°ì´í„°ê°€ ë¶€ì¡±í•œ ê²½ìš° ì„ í˜• ë³´ê°„
            return series.interpolate(method='linear')
        
        try:
            # ì¼ì‹œì ìœ¼ë¡œ ê²°ì¸¡ì¹˜ë¥¼ ì„ í˜• ë³´ê°„ìœ¼ë¡œ ì±„ì›€
            temp_series = series.interpolate(method='linear')
            
            # ì‹œê³„ì—´ ë¶„í•´
            decomposition = seasonal_decompose(temp_series, model='additive', period=period)
            
            # ê° ì„±ë¶„ë³„ë¡œ ê²°ì¸¡ì¹˜ ë³´ê°„
            trend_filled = decomposition.trend.interpolate(method='linear')
            seasonal_filled = decomposition.seasonal.fillna(method='ffill').fillna(method='bfill')
            resid_filled = decomposition.resid.fillna(0)  # ì”ì°¨ëŠ” 0ìœ¼ë¡œ ì±„ì›€
            
            # ì„±ë¶„ ì¬ê²°í•©
            reconstructed = trend_filled + seasonal_filled + resid_filled
            
            # ì›ë˜ ê²°ì¸¡ì¹˜ ìœ„ì¹˜ì—ë§Œ ì¬êµ¬ì„±ëœ ê°’ ì‚¬ìš©
            result = series.copy()
            result.loc[series.isnull()] = reconstructed.loc[series.isnull()]
            
            return result
            
        except Exception as e:
            print(f"ì‹œê³„ì—´ ë¶„í•´ ì‹¤íŒ¨: {e}, ì„ í˜• ë³´ê°„ ì‚¬ìš©")
            return series.interpolate(method='linear')
    
    def _handle_outliers(self, data: pd.DataFrame, target_columns: List[str]) -> pd.DataFrame:
        """ì´ìƒì¹˜ íƒì§€ ë° ì²˜ë¦¬"""
        print("\nğŸ¯ ì´ìƒì¹˜ ì²˜ë¦¬ ì¤‘...")
        
        method = self.config['outlier_detection']['method']
        threshold = self.config['outlier_detection']['threshold']
        action = self.config['outlier_detection']['action']
        
        total_outliers = 0
        
        for column in target_columns:
            series = data[column].dropna()
            if len(series) == 0:
                continue
                
            # ì´ìƒì¹˜ íƒì§€
            if method == 'iqr':
                outlier_mask = self._detect_outliers_iqr(series, threshold)
            elif method == 'zscore':
                outlier_mask = self._detect_outliers_zscore(series, threshold)
            elif method == 'modified_zscore':
                outlier_mask = self._detect_outliers_modified_zscore(series, threshold)
            else:
                # ê¸°ë³¸ê°’: IQR
                outlier_mask = self._detect_outliers_iqr(series, threshold)
            
            outlier_count = outlier_mask.sum()
            total_outliers += outlier_count
            
            if outlier_count > 0:
                # ì´ìƒì¹˜ ì²˜ë¦¬
                if action == 'remove':
                    data = data.loc[~outlier_mask]
                elif action == 'cap':
                    data.loc[outlier_mask, column] = self._cap_outliers(series, outlier_mask)
                elif action == 'interpolate':
                    data.loc[outlier_mask, column] = np.nan
                    data[column] = data[column].interpolate(method='linear')
        
        print(f"íƒì§€ëœ ì´ìƒì¹˜: {total_outliers:,}ê°œ")
        print(f"âœ… ì´ìƒì¹˜ ì²˜ë¦¬ ì™„ë£Œ ({method}-{action} ë°©ë²• ì‚¬ìš©)")
        
        self.preprocessing_history.append(f"ì´ìƒì¹˜ ì²˜ë¦¬: {method}-{action}")
        
        return data
    
    def _detect_outliers_iqr(self, series: pd.Series, threshold: float = 1.5) -> pd.Series:
        """IQR ë°©ë²•ìœ¼ë¡œ ì´ìƒì¹˜ íƒì§€"""
        Q1 = series.quantile(0.25)
        Q3 = series.quantile(0.75)
        IQR = Q3 - Q1
        
        lower_bound = Q1 - threshold * IQR
        upper_bound = Q3 + threshold * IQR
        
        return (series < lower_bound) | (series > upper_bound)
    
    def _detect_outliers_zscore(self, series: pd.Series, threshold: float = 3.0) -> pd.Series:
        """Z-score ë°©ë²•ìœ¼ë¡œ ì´ìƒì¹˜ íƒì§€"""
        z_scores = np.abs(stats.zscore(series))
        return z_scores > threshold
    
    def _detect_outliers_modified_zscore(self, series: pd.Series, threshold: float = 3.5) -> pd.Series:
        """Modified Z-score ë°©ë²•ìœ¼ë¡œ ì´ìƒì¹˜ íƒì§€"""
        median = series.median()
        mad = np.median(np.abs(series - median))
        modified_z_scores = 0.6745 * (series - median) / mad
        return np.abs(modified_z_scores) > threshold
    
    def _cap_outliers(self, series: pd.Series, outlier_mask: pd.Series) -> pd.Series:
        """ì´ìƒì¹˜ë¥¼ ê²½ê³„ê°’ìœ¼ë¡œ ì œí•œ"""
        Q1 = series.quantile(0.25)
        Q3 = series.quantile(0.75)
        IQR = Q3 - Q1
        
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        capped_values = series.copy()
        capped_values[outlier_mask] = np.where(
            series[outlier_mask] < lower_bound, 
            lower_bound, 
            upper_bound
        )
        
        return capped_values[outlier_mask]
    
    def _apply_noise_filtering(self, data: pd.DataFrame, target_columns: List[str]) -> pd.DataFrame:
        """ë…¸ì´ì¦ˆ í•„í„°ë§ ì ìš©"""
        print("\nğŸ”„ ë…¸ì´ì¦ˆ í•„í„°ë§ ì¤‘...")
        
        method = self.config['noise_filtering']['method']
        
        for column in target_columns:
            if method == 'moving_average':
                window_size = self.config['noise_filtering']['window_size']
                data[column] = data[column].rolling(window=window_size, center=True).mean().fillna(data[column])
            
            elif method == 'exponential_smoothing':
                alpha = self.config['noise_filtering']['alpha']
                data[column] = data[column].ewm(alpha=alpha).mean()
            
            elif method == 'kalman':
                data[column] = self._kalman_filter(data[column])
        
        print(f"âœ… ë…¸ì´ì¦ˆ í•„í„°ë§ ì™„ë£Œ ({method} ë°©ë²• ì‚¬ìš©)")
        
        self.preprocessing_history.append(f"ë…¸ì´ì¦ˆ í•„í„°ë§: {method}")
        
        return data
    
    def _kalman_filter(self, series: pd.Series) -> pd.Series:
        """1ì°¨ì› ì¹¼ë§Œ í•„í„° ì ìš©"""
        # ê°„ë‹¨í•œ 1ì°¨ì› ì¹¼ë§Œ í•„í„° êµ¬í˜„
        n = len(series)
        filtered = np.zeros(n)
        
        # ì´ˆê¸°ê°’
        x = series.iloc[0]  # ì´ˆê¸° ìƒíƒœ
        P = 1.0  # ì´ˆê¸° ê³µë¶„ì‚°
        Q = 0.1  # í”„ë¡œì„¸ìŠ¤ ë…¸ì´ì¦ˆ
        R = 1.0  # ì¸¡ì • ë…¸ì´ì¦ˆ
        
        for i in range(n):
            # ì˜ˆì¸¡ ë‹¨ê³„
            x_pred = x
            P_pred = P + Q
            
            # ì—…ë°ì´íŠ¸ ë‹¨ê³„
            if not pd.isna(series.iloc[i]):
                K = P_pred / (P_pred + R)  # ì¹¼ë§Œ ì´ë“
                x = x_pred + K * (series.iloc[i] - x_pred)
                P = (1 - K) * P_pred
            else:
                x = x_pred
                P = P_pred
            
            filtered[i] = x
        
        return pd.Series(filtered, index=series.index)
    
    def _feature_engineering(self, data: pd.DataFrame, target_columns: List[str]) -> pd.DataFrame:
        """í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§"""
        print("\nâš™ï¸ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì¤‘...")
        
        original_columns = data.columns.tolist()
        feature_config = self.config['feature_engineering']
        
        # ì‹œê°„ ì»¬ëŸ¼ ì°¾ê¸°
        time_col = None
        for col in ['datetime', 'timestamp', 'time']:
            if col in data.columns:
                time_col = col
                break
        
        # ì‹œê°„ íŠ¹ì„± ì¶”ê°€
        if feature_config['time_features'] and time_col:
            data = self._add_time_features(data, time_col)
        
        # ì§€ì—° íŠ¹ì„± ì¶”ê°€
        if feature_config['lag_features']:
            data = self._add_lag_features(data, target_columns, feature_config['lag_periods'])
        
        # ì´ë™í†µê³„ íŠ¹ì„± ì¶”ê°€
        if feature_config['rolling_features']:
            data = self._add_rolling_features(data, target_columns, feature_config['rolling_windows'])
        
        # ìƒˆë¡œ ìƒì„±ëœ í”¼ì²˜ ì´ë¦„ ì €ì¥
        new_features = [col for col in data.columns if col not in original_columns]
        self.feature_names.extend(new_features)
        
        print(f"âœ… í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ: {len(new_features)}ê°œ í”¼ì²˜ ìƒì„±")
        
        self.preprocessing_history.append(f"í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§: {len(new_features)}ê°œ í”¼ì²˜ ìƒì„±")
        
        return data
    
    def _add_time_features(self, data: pd.DataFrame, time_col: str) -> pd.DataFrame:
        """ì‹œê°„ ê´€ë ¨ í”¼ì²˜ ì¶”ê°€"""
        # localtime ì»¬ëŸ¼ ìš°ì„  ì‚¬ìš©
        if 'localtime' in data.columns:
            dt_series = data['localtime']
        else:
            dt_series = pd.to_datetime(data[time_col])
        
        # ê¸°ë³¸ ì‹œê°„ íŠ¹ì„±
        data['hour'] = dt_series.dt.hour
        data['day_of_week'] = dt_series.dt.dayofweek
        data['day_of_month'] = dt_series.dt.day
        data['month'] = dt_series.dt.month
        data['quarter'] = dt_series.dt.quarter
        data['year'] = dt_series.dt.year
        
        # ìˆœí™˜ íŠ¹ì„± (ì‚¼ê°í•¨ìˆ˜ë¡œ ì¸ì½”ë”©)
        data['hour_sin'] = np.sin(2 * np.pi * data['hour'] / 24)
        data['hour_cos'] = np.cos(2 * np.pi * data['hour'] / 24)
        data['day_sin'] = np.sin(2 * np.pi * data['day_of_week'] / 7)
        data['day_cos'] = np.cos(2 * np.pi * data['day_of_week'] / 7)
        data['month_sin'] = np.sin(2 * np.pi * data['month'] / 12)
        data['month_cos'] = np.cos(2 * np.pi * data['month'] / 12)
        
        # íŠ¹ë³„í•œ ë‚  íŠ¹ì„±
        data['is_weekend'] = (data['day_of_week'] >= 5).astype(int)
        data['is_business_hour'] = ((data['hour'] >= 9) & (data['hour'] <= 17)).astype(int)
        
        # ê³µì¥ ìš´ì˜ íŠ¹ì„± (ìŠ¤ë§ˆíŠ¸ íŒ©í† ë¦¬ íŠ¹í™”)
        data['is_peak_hour'] = ((data['hour'] >= 8) & (data['hour'] <= 18)).astype(int)  # ì£¼ìš” ìƒì‚°ì‹œê°„
        data['is_night_shift'] = ((data['hour'] >= 22) | (data['hour'] <= 6)).astype(int)  # ì•¼ê°„ ê·¼ë¬´
        
        print(f"ì‹œê°„ íŠ¹ì„± ì¶”ê°€ ì™„ë£Œ: {12}ê°œ í”¼ì²˜ ìƒì„±")
        
        return data
    
    def _add_lag_features(self, data: pd.DataFrame, target_columns: List[str], lag_periods: List[int]) -> pd.DataFrame:
        """ì§€ì—° íŠ¹ì„± ì¶”ê°€"""
        for column in target_columns:
            for lag in lag_periods:
                lag_col_name = f"{column}_lag_{lag}"
                data[lag_col_name] = data[column].shift(lag)
        
        return data
    
    def _add_rolling_features(self, data: pd.DataFrame, target_columns: List[str], windows: List[int]) -> pd.DataFrame:
        """ì´ë™í†µê³„ íŠ¹ì„± ì¶”ê°€"""
        for column in target_columns:
            for window in windows:
                # ì´ë™í‰ê· 
                data[f"{column}_rolling_mean_{window}"] = data[column].rolling(window=window).mean()
                
                # ì´ë™í‘œì¤€í¸ì°¨
                data[f"{column}_rolling_std_{window}"] = data[column].rolling(window=window).std()
                
                # ì´ë™ìµœì†Ÿê°’/ìµœëŒ“ê°’
                data[f"{column}_rolling_min_{window}"] = data[column].rolling(window=window).min()
                data[f"{column}_rolling_max_{window}"] = data[column].rolling(window=window).max()
                
                # ì´ë™ë²”ìœ„
                data[f"{column}_rolling_range_{window}"] = (
                    data[f"{column}_rolling_max_{window}"] - data[f"{column}_rolling_min_{window}"]
                )
        
        return data
    
    def _apply_scaling(self, data: pd.DataFrame, target_columns: List[str]) -> pd.DataFrame:
        """ìŠ¤ì¼€ì¼ë§ ì ìš©"""
        print("\nìŠ¤ì¼€ì¼ë§ ì¤‘..")
        
        method = self.config['scaling']['method']
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì´ˆê¸°í™”
        if method == 'standard':
            scaler = StandardScaler()
        elif method == 'minmax':
            feature_range = self.config['scaling']['feature_range']
            scaler = MinMaxScaler(feature_range=feature_range)
        elif method == 'robust':
            scaler = RobustScaler()
        else:
            print(f"ì•Œ ìˆ˜ ì—†ëŠ” ìŠ¤ì¼€ì¼ë§ ë°©ë²•: {method}")
            return data
        
        # ìŠ¤ì¼€ì¼ë§ ì ìš©í•  ì»¬ëŸ¼ (ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ)
        numeric_columns = data.select_dtypes(include=[np.number]).columns.tolist()
        # ì‹œê°„ ê´€ë ¨ ì»¬ëŸ¼ ì œì™¸
        time_related_cols = ['year', 'month', 'day', 'hour', 'minute', 'weekday', 'quarter', 'is_weekend', 'is_business_hour']
        scaling_columns = [col for col in numeric_columns if col not in time_related_cols]
        
        if len(scaling_columns) > 0:
            # ê²°ì¸¡ì¹˜ê°€ ì—†ëŠ” ë°ì´í„°ë¡œ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ
            clean_data = data[scaling_columns].dropna()
            if len(clean_data) > 0:
                scaler.fit(clean_data)
                
                # ì „ì²´ ë°ì´í„°ì— ìŠ¤ì¼€ì¼ë§ ì ìš© (ê²°ì¸¡ì¹˜ ì œì™¸)
                mask = data[scaling_columns].notna().all(axis=1)
                data.loc[mask, scaling_columns] = scaler.transform(data.loc[mask, scaling_columns])
                
                # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
                self.scalers[method] = scaler
                
                print(f"ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ ({method} ë°©ë²• ì‚¬ìš©, {len(scaling_columns)}ê°œ ì»¬ëŸ¼)")
        
        self.preprocessing_history.append(f"ìŠ¤ì¼€ì¼ë§: {method}")
        
        return data
    
    def _print_preprocessing_summary(self, original_data: pd.DataFrame, processed_data: pd.DataFrame):
        """ì „ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*50)
        print("ğŸ“Š ì „ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½")
        print("="*50)
        
        print(f"ì›ë³¸ ë°ì´í„° í¬ê¸°: {original_data.shape}")
        print(f"ì²˜ë¦¬ í›„ ë°ì´í„° í¬ê¸°: {processed_data.shape}")
        print(f"ìƒˆë¡œ ìƒì„±ëœ í”¼ì²˜: {len(self.feature_names)}ê°œ")
        
        print(f"\nì „ì²˜ë¦¬ ë‹¨ê³„:")
        for i, step in enumerate(self.preprocessing_history, 1):
            print(f"  {i}. {step}")
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¹„êµ
        original_memory = original_data.memory_usage(deep=True).sum() / 1024**2
        processed_memory = processed_data.memory_usage(deep=True).sum() / 1024**2
        
        print(f"\në©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰:")
        print(f"  ì›ë³¸: {original_memory:.2f} MB")
        print(f"  ì²˜ë¦¬ í›„: {processed_memory:.2f} MB")
        print(f"  ì¦ê°€ìœ¨: {(processed_memory/original_memory-1)*100:.1f}%")
    
    def visualize_preprocessing_results(self, original_data: pd.DataFrame, 
                                      processed_data: pd.DataFrame,
                                      sample_columns: List[str] = None):
        """ì „ì²˜ë¦¬ ê²°ê³¼ ì‹œê°í™”"""
        print("\nğŸ“ˆ ì „ì²˜ë¦¬ ê²°ê³¼ ì‹œê°í™”")
        
        if sample_columns is None:
            # ìˆ«ìí˜• ì»¬ëŸ¼ ì¤‘ ì²˜ìŒ 4ê°œ ì„ íƒ
            numeric_cols = original_data.select_dtypes(include=[np.number]).columns.tolist()
            sample_columns = numeric_cols[:4]
        
        fig, axes = plt.subplots(len(sample_columns), 2, figsize=(20, 5*len(sample_columns)))
        if len(sample_columns) == 1:
            axes = axes.reshape(1, -1)
        
        for i, column in enumerate(sample_columns):
            if column in original_data.columns and column in processed_data.columns:
                # ì›ë³¸ ë°ì´í„°
                axes[i, 0].plot(original_data[column], alpha=0.7, label='ì›ë³¸ ë°ì´í„°')
                axes[i, 0].set_title(f'{column} - ì›ë³¸ ë°ì´í„°', fontweight='bold', fontsize=14)
                axes[i, 0].set_ylabel('ê°’', fontsize=12)
                axes[i, 0].grid(True, alpha=0.3)
                axes[i, 0].legend(fontsize=10)
                
                # ì²˜ë¦¬ëœ ë°ì´í„°
                axes[i, 1].plot(processed_data[column], alpha=0.7, label='ì „ì²˜ë¦¬ ì™„ë£Œ', color='orange')
                axes[i, 1].set_title(f'{column} - ì „ì²˜ë¦¬ ì™„ë£Œ ë°ì´í„°', fontweight='bold', fontsize=14)
                axes[i, 1].set_ylabel('ê°’', fontsize=12)
                axes[i, 1].grid(True, alpha=0.3)
                axes[i, 1].legend(fontsize=10)
        
        plt.tight_layout()
        
        # reports ë””ë ‰í† ë¦¬ ìƒì„±
        reports_dir = project_root / "reports"
        reports_dir.mkdir(exist_ok=True)
        
        # ì´ë¯¸ì§€ ì €ì¥
        save_path = reports_dir / "preprocessing_comparison.png"
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"ì‹œê°í™” ê²°ê³¼ ì €ì¥: {save_path}")
        
        plt.show()
    
    def save_preprocessed_data(self, data: pd.DataFrame, filename: str = None):
        """ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"preprocessed_data_{timestamp}.csv"
        
        output_path = project_root / "data" / "processed" / filename
        
        # ë””ë ‰í† ë¦¬ ìƒì„±
        output_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ë°ì´í„° ì €ì¥
        data.to_csv(output_path, index=False)
        
        print(f"ì „ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_path}")
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            'original_shape': data.shape,
            'preprocessing_steps': self.preprocessing_history,
            'new_features': self.feature_names,
            'config': self.config,
            'timestamp': datetime.now().isoformat()
        }
        
        import json
        metadata_path = output_path.with_suffix('.json')
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"ë©”íƒ€ë°ì´í„° ì €ì¥ ì™„ë£Œ: {metadata_path}")
        
        return output_path
    
    def get_feature_importance_data(self, data: pd.DataFrame, target_column: str) -> pd.DataFrame:
        """í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° ì¤€ë¹„"""
        from sklearn.ensemble import RandomForestRegressor
        from sklearn.feature_selection import mutual_info_regression
        
        # ìˆ«ìí˜• í”¼ì²˜ë§Œ ì„ íƒ
        numeric_features = data.select_dtypes(include=[np.number]).columns.tolist()
        if target_column in numeric_features:
            numeric_features.remove(target_column)
        
        # ê²°ì¸¡ì¹˜ ì œê±°
        clean_data = data[numeric_features + [target_column]].dropna()
        
        if len(clean_data) == 0:
            print("í”¼ì²˜ ì¤‘ìš”ë„ ë¶„ì„ì„ ìœ„í•œ ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            return pd.DataFrame()
        
        X = clean_data[numeric_features]
        y = clean_data[target_column]
        
        # Random Forest í”¼ì²˜ ì¤‘ìš”ë„
        rf = RandomForestRegressor(n_estimators=100, random_state=42)
        rf.fit(X, y)
        rf_importance = rf.feature_importances_
        
        # ìƒí˜¸ì •ë³´ëŸ‰
        mi_scores = mutual_info_regression(X, y, random_state=42)
        
        # ê²°ê³¼ ì •ë¦¬
        importance_df = pd.DataFrame({
            'feature': numeric_features,
            'rf_importance': rf_importance,
            'mutual_info': mi_scores
        })
        
        # ì •ê·œí™”
        importance_df['rf_importance_norm'] = importance_df['rf_importance'] / importance_df['rf_importance'].max()
        importance_df['mutual_info_norm'] = importance_df['mutual_info'] / importance_df['mutual_info'].max()
        
        # í‰ê·  ì¤‘ìš”ë„ ê³„ì‚°
        importance_df['avg_importance'] = (importance_df['rf_importance_norm'] + importance_df['mutual_info_norm']) / 2
        
        return importance_df.sort_values('avg_importance', ascending=False)


class QuickPreprocessor:
    """
    ë¹ ë¥¸ ì „ì²˜ë¦¬ë¥¼ ìœ„í•œ ê²½ëŸ‰í™” í´ë˜ìŠ¤
    ëŒ€ìš©ëŸ‰ ë°ì´í„°ì— ëŒ€í•œ ê¸°ë³¸ì ì¸ ì „ì²˜ë¦¬ë§Œ ìˆ˜í–‰
    """
    
    @staticmethod
    def quick_clean(data: pd.DataFrame) -> pd.DataFrame:
        """ê¸°ë³¸ì ì¸ ì •ë¦¬ë§Œ ìˆ˜í–‰"""
        print("âš¡ ë¹ ë¥¸ ì „ì²˜ë¦¬ ì‹œì‘...")
        
        # 1. ì¤‘ë³µ ì œê±°
        before_dup = len(data)
        data = data.drop_duplicates()
        after_dup = len(data)
        print(f"ì¤‘ë³µ ì œê±°: {before_dup - after_dup}ê°œ í–‰ ì œê±°")
        
        # 2. ê²°ì¸¡ì¹˜ê°€ 50% ì´ìƒì¸ ì»¬ëŸ¼ ì œê±°
        threshold = 0.5
        missing_ratio = data.isnull().sum() / len(data)
        cols_to_drop = missing_ratio[missing_ratio > threshold].index.tolist()
        if cols_to_drop:
            data = data.drop(columns=cols_to_drop)
            print(f"ê²°ì¸¡ì¹˜ ë§ì€ ì»¬ëŸ¼ ì œê±°: {len(cols_to_drop)}ê°œ")
        
        # 3. ìƒìˆ˜ ì»¬ëŸ¼ ì œê±°
        numeric_cols = data.select_dtypes(include=[np.number]).columns
        constant_cols = []
        for col in numeric_cols:
            if data[col].nunique() <= 1:
                constant_cols.append(col)
        
        if constant_cols:
            data = data.drop(columns=constant_cols)
            print(f"ìƒìˆ˜ ì»¬ëŸ¼ ì œê±°: {len(constant_cols)}ê°œ")
        
        # 4. ê¸°ë³¸ ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        for col in data.select_dtypes(include=[np.number]).columns:
            data[col] = data[col].fillna(data[col].median())
        
        print(f"ë¹ ë¥¸ ì „ì²˜ë¦¬ ì™„ë£Œ: {data.shape}")
        
        return data
    
    @staticmethod
    def add_basic_time_features(data: pd.DataFrame, time_col: str) -> pd.DataFrame:
        """ê¸°ë³¸ ì‹œê°„ í”¼ì²˜ë§Œ ì¶”ê°€"""
        dt_series = pd.to_datetime(data[time_col])
        
        data['hour'] = dt_series.dt.hour
        data['day_of_week'] = dt_series.dt.dayofweek
        data['month'] = dt_series.dt.month
        data['is_weekend'] = (data['day_of_week'] >= 5).astype(int)
        
        return data


if __name__ == "__main__":
    # ì „ì²˜ë¦¬ ì˜ˆì œ ì‹¤í–‰
    print("ğŸ”§ ì „ì²˜ë¦¬ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
    
    # ì„¤ì • ì˜ˆì œ
    config = {
        'missing_value': {'method': 'seasonal_decomposition'},
        'outlier_detection': {'method': 'iqr', 'action': 'cap'},
        'noise_filtering': {'method': 'moving_average'},
        'scaling': {'method': 'robust'},
        'feature_engineering': {
            'time_features': True,
            'lag_features': True,
            'rolling_features': True,
            'lag_periods': [1, 6, 24],
            'rolling_windows': [6, 24]
        }
    }
    
    print("ì „ì²˜ë¦¬ ëª¨ë“ˆ ì¤€ë¹„ ì™„ë£Œ") 
"""
ìŠ¤ë§ˆíŠ¸ íŒ©í† ë¦¬ ì „ë ¥ ì˜ˆì¸¡ - ML/DL í•™ìŠµ ë°ì´í„° ì¤€ë¹„
================================================

ì „ì²˜ë¦¬ëœ ë°ì´í„°ë¥¼ ML/DL ëª¨ë¸ í•™ìŠµì— ì í•©í•œ í˜•íƒœë¡œ ë³€í™˜
- ì‹œê³„ì—´ ë°ì´í„° êµ¬ì¡°í™”
- Train/Validation/Test ë¶„í• 
- Feature/Target ë¶„ë¦¬
- ìµœì¢… í•™ìŠµìš© ë°ì´í„° ìƒì„±
"""

import pandas as pd
import numpy as np
import os
import sys
from pathlib import Path
from datetime import datetime
from typing import Tuple, Dict, List
import warnings
warnings.filterwarnings('ignore')

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import json

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ì„¤ì •
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))

class MLDataPreparator:
    """ML/DL í•™ìŠµìš© ë°ì´í„° ì¤€ë¹„ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.feature_columns = []
        self.target_column = None
        self.scaler = None
        self.data_info = {}
        self.train_feature_stats = {}  # í›ˆë ¨ ë°ì´í„° í†µê³„ ì €ì¥
        
    def load_preprocessed_data(self, file_path: str = None) -> pd.DataFrame:
        """ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë“œ"""
        if file_path is None:
            # ê°€ì¥ ìµœê·¼ ì „ì²˜ë¦¬ íŒŒì¼ ì°¾ê¸°
            processed_dir = project_root / "data" / "processed"
            csv_files = list(processed_dir.glob("preprocessed_data_*.csv"))
            if not csv_files:
                raise FileNotFoundError("ì „ì²˜ë¦¬ëœ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            file_path = max(csv_files, key=lambda x: x.stat().st_mtime)
        
        print(f"ì „ì²˜ë¦¬ëœ ë°ì´í„° ë¡œë”©: {file_path}")
        data = pd.read_csv(file_path)
        
        print(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {data.shape}")
        print(f"   - ì»¬ëŸ¼ ìˆ˜: {len(data.columns)}")
        print(f"   - ë©”ëª¨ë¦¬: {data.memory_usage(deep=True).sum() / 1024**2:.2f} MB")
        
        return data
    
    def identify_target_and_features(self, data: pd.DataFrame) -> Tuple[str, List[str]]:
        """íƒ€ê²Ÿ ë³€ìˆ˜ì™€ í”¼ì²˜ ë³€ìˆ˜ ì‹ë³„"""
        print("\níƒ€ê²Ÿ ë³€ìˆ˜ ë° í”¼ì²˜ ì‹ë³„")
        
        # ì „ë ¥ ê´€ë ¨ ì»¬ëŸ¼ë“¤ ì°¾ê¸°
        power_cols = [col for col in data.columns if any(keyword in col.lower() 
                     for keyword in ['power', 'energy', 'voltage', 'current'])]
        
        print(f"ì „ë ¥ ê´€ë ¨ ì»¬ëŸ¼: {power_cols}")
        
        # íƒ€ê²Ÿ ë³€ìˆ˜ ì„ íƒ (activePowerë¥¼ ìš°ì„ ì ìœ¼ë¡œ)
        target_candidates = ['activePower', 'hourly_pow', 'totalActivePower']
        target_column = None
        
        for candidate in target_candidates:
            if candidate in data.columns:
                target_column = candidate
                break
        
        if target_column is None and power_cols:
            target_column = power_cols[0]  # ì²« ë²ˆì§¸ ì „ë ¥ ì»¬ëŸ¼ ì‚¬ìš©
        
        if target_column is None:
            raise ValueError("íƒ€ê²Ÿ ë³€ìˆ˜ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # í”¼ì²˜ ë³€ìˆ˜ ì„ íƒ (íƒ€ê²Ÿ ì œì™¸, í…ìŠ¤íŠ¸ ì»¬ëŸ¼ ì œì™¸)
        exclude_cols = [target_column, 'localtime', 'timestamp', 'equipmentName']
        feature_columns = [col for col in data.columns 
                          if col not in exclude_cols and data[col].dtype in ['int64', 'float64']]
        
        print(f"íƒ€ê²Ÿ ë³€ìˆ˜: {target_column}")
        print(f"í”¼ì²˜ ë³€ìˆ˜: {len(feature_columns)}ê°œ")
        print(f"   ì£¼ìš” í”¼ì²˜: {feature_columns[:10]}...")
        
        self.target_column = target_column
        self.feature_columns = feature_columns
        
        return target_column, feature_columns
    
    def create_time_series_sequences(self, data: pd.DataFrame, 
                                   sequence_length: int = 24,
                                   prediction_horizon: int = 1) -> Tuple[np.ndarray, np.ndarray]:
        """ì‹œê³„ì—´ ì˜ˆì¸¡ì„ ìœ„í•œ ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±"""
        print(f"\nì‹œê³„ì—´ ì‹œí€€ìŠ¤ ìƒì„± (ìœˆë„ìš°: {sequence_length}, ì˜ˆì¸¡: {prediction_horizon})")
        
        # ì‹œê°„ ìˆœì„œëŒ€ë¡œ ì •ë ¬
        if 'localtime' in data.columns:
            data = data.sort_values('localtime').reset_index(drop=True)
        
        # í”¼ì²˜ì™€ íƒ€ê²Ÿ ë°ì´í„° ì¤€ë¹„
        features = data[self.feature_columns].values
        targets = data[self.target_column].values
        
        X_sequences = []
        y_sequences = []
        
        for i in range(len(data) - sequence_length - prediction_horizon + 1):
            # ì…ë ¥ ì‹œí€€ìŠ¤ (ê³¼ê±° sequence_length ì‹œì )
            X_seq = features[i:i + sequence_length]
            
            # íƒ€ê²Ÿ (prediction_horizon ì‹œì  í›„)
            y_seq = targets[i + sequence_length:i + sequence_length + prediction_horizon]
            
            X_sequences.append(X_seq)
            y_sequences.append(y_seq)
        
        X_sequences = np.array(X_sequences)
        y_sequences = np.array(y_sequences)
        
        print(f"ì‹œí€€ìŠ¤ ìƒì„± ì™„ë£Œ:")
        print(f"   - X shape: {X_sequences.shape} (samples, timesteps, features)")
        print(f"   - y shape: {y_sequences.shape} (samples, prediction_horizon)")
        
        return X_sequences, y_sequences
    
    def create_tabular_data(self, data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """í…Œì´ë¸” í˜•íƒœ MLì„ ìœ„í•œ ë°ì´í„° ìƒì„±"""
        print(f"\ní…Œì´ë¸” í˜•íƒœ ë°ì´í„° ìƒì„±")
        
        # ê²°ì¸¡ì¹˜ ì œê±°
        clean_data = data[self.feature_columns + [self.target_column]].dropna()
        
        X = clean_data[self.feature_columns].values
        y = clean_data[self.target_column].values
        
        # í›ˆë ¨ ë°ì´í„° í†µê³„ ì €ì¥ (í…ŒìŠ¤íŠ¸ ë°ì´í„° ì²˜ë¦¬ìš©)
        self.train_feature_stats = {}
        for i, feature in enumerate(self.feature_columns):
            self.train_feature_stats[feature] = {
                'mean': float(clean_data[feature].mean()),
                'std': float(clean_data[feature].std()),
                'min': float(clean_data[feature].min()),
                'max': float(clean_data[feature].max())
            }
        
        print(f"í…Œì´ë¸” ë°ì´í„° ìƒì„± ì™„ë£Œ:")
        print(f"   - X shape: {X.shape} (samples, features)")
        print(f"   - y shape: {y.shape} (samples,)")
        print(f"   - ê²°ì¸¡ì¹˜ ì œê±° í›„: {len(clean_data)}/{len(data)} ìƒ˜í”Œ")
        print(f"   - í›ˆë ¨ ë°ì´í„° í†µê³„ ì €ì¥: {len(self.train_feature_stats)}ê°œ í”¼ì²˜")
        
        return X, y
    
    def split_data(self, X: np.ndarray, y: np.ndarray, 
                   val_size: float = 0.2,
                   time_series: bool = True) -> Dict:
        """ë°ì´í„° ë¶„í•  (Train/Validationë§Œ, TestëŠ” ë³„ë„ íŒŒì¼)"""
        print(f"\nTrain ë°ì´í„° ë¶„í•  (Validation: {val_size*100}%)")
        
        if time_series:
            # ì‹œê³„ì—´ ë°ì´í„°ëŠ” ì‹œê°„ ìˆœì„œ ìœ ì§€
            n_samples = len(X)
            n_val = int(n_samples * val_size)
            
            # ì‹œê°„ ìˆœì„œëŒ€ë¡œ ë¶„í•  (ë§ˆì§€ë§‰ ë¶€ë¶„ì„ validationìœ¼ë¡œ)
            X_train = X[:-n_val]
            y_train = y[:-n_val]
            X_val = X[-n_val:]
            y_val = y[-n_val:]
            
        else:
            # ì¼ë°˜ MLì€ ëœë¤ ë¶„í• 
            X_train, X_val, y_train, y_val = train_test_split(
                X, y, test_size=val_size, random_state=42
            )
        
        split_data = {
            'X_train': X_train, 'y_train': y_train,
            'X_val': X_val, 'y_val': y_val
        }
        
        print(f"Train ë°ì´í„° ë¶„í•  ì™„ë£Œ:")
        print(f"   - Train: {X_train.shape[0]} ìƒ˜í”Œ")
        print(f"   - Validation: {X_val.shape[0]} ìƒ˜í”Œ")
        
        return split_data
    
    def prepare_test_data(self, submission_template: pd.DataFrame, apply_scaling: bool = True) -> Tuple[np.ndarray, pd.DataFrame]:
        """ì‹¤ì œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ (submission í…œí”Œë¦¿ì— ë§ì¶¤, í›ˆë ¨ ë°ì´í„°ì™€ ë™ì¼í•œ ì „ì²˜ë¦¬ ì ìš©)"""
        print(f"\nì‹¤ì œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ (í›ˆë ¨ ë°ì´í„°ì™€ ë™ì¼í•œ ì „ì²˜ë¦¬)")
        
        # test.csv ë¡œë“œ
        test_path = project_root / "data" / "raw" / "test.csv"
        if not test_path.exists():
            raise FileNotFoundError(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {test_path}")
        
        print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ë¡œë”©: {test_path}")
        
        # Submission ê¸°ê°„ í™•ì¸
        submission_times = pd.to_datetime(submission_template['id'])
        start_time = submission_times.min()
        end_time = submission_times.max()
        
        print(f"Submission ì˜ˆì¸¡ ê¸°ê°„: {start_time} ~ {end_time}")
        print(f"   ì´ {len(submission_template)}ê°œ ì‹œì  ì˜ˆì¸¡ í•„ìš”")
        
        # íš¨ìœ¨ì  ë¡œë”©: í•„ìš”í•œ ë¶€ë¶„ + ì—¬ìœ ë¶„ë§Œ ë¡œë“œ
        print(f"íš¨ìœ¨ì  ë°ì´í„° ë¡œë”© ì¤‘...")
        
        # ì „ì²´ íŒŒì¼ì—ì„œ ìƒ˜í”Œë§ (ì‹œê³„ì—´ ì—°ì†ì„± ê³ ë ¤í•˜ì—¬ ìµœê·¼ ë°ì´í„°)
        total_rows_estimate = 500000  # ëŒ€ëµì  ì¶”ì •
        skip_rows = max(0, total_rows_estimate - 50000)  # ë§ˆì§€ë§‰ 5ë§Œê°œë§Œ ë¡œë“œ
        
        test_data = pd.read_csv(test_path, skiprows=range(1, skip_rows), nrows=50000)
        print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ìƒ˜í”Œ ë¡œë“œ: {test_data.shape}")
        
        # ì „ì²´ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì ìš© (í›ˆë ¨ ë°ì´í„°ì™€ ë™ì¼)
        print(f"ì „ì²´ ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì ìš© ì¤‘...")
        
        # 1. localtime ì²˜ë¦¬
        if 'localtime' in test_data.columns:
            test_data['localtime'] = pd.to_datetime(test_data['localtime'].astype(str), format='%Y%m%d%H%M%S', errors='coerce')
            test_data = test_data.sort_values('localtime').reset_index(drop=True)
            print(f"ì‹œê°„ ì»¬ëŸ¼ ì²˜ë¦¬ ì™„ë£Œ")
        
        # 2. ê²°ì¸¡ì¹˜ ì²˜ë¦¬
        numeric_cols = test_data.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            if test_data[col].isnull().sum() > 0:
                test_data[col] = test_data[col].fillna(test_data[col].median())
        print(f"ê²°ì¸¡ì¹˜ ì²˜ë¦¬ ì™„ë£Œ")
        
        # 3. ì‹œê°„ íŠ¹ì„± ì¶”ê°€ (Trainê³¼ ë™ì¼ êµ¬ì¡° í•„ìš”)
        if 'localtime' in test_data.columns:
            dt_series = test_data['localtime']
            
            # ê¸°ë³¸ ì‹œê°„ íŠ¹ì„±
            test_data['hour'] = dt_series.dt.hour
            test_data['day_of_week'] = dt_series.dt.dayofweek
            test_data['month'] = dt_series.dt.month
            
            # ìŠ¤ë§ˆíŠ¸ íŒ©í† ë¦¬ íŠ¹í™” íŠ¹ì„±
            test_data['is_weekend'] = (test_data['day_of_week'] >= 5).astype(int)
            test_data['is_peak_hour'] = ((test_data['hour'] >= 8) & (test_data['hour'] <= 18)).astype(int)
            test_data['is_business_hour'] = ((test_data['hour'] >= 9) & (test_data['hour'] <= 17)).astype(int)
            
            print(f"ì‹œê°„ íŠ¹ì„± ì¶”ê°€ ì™„ë£Œ")
        
        # 4. Submission ê¸°ê°„ì— í•´ë‹¹í•˜ëŠ” ë°ì´í„° ì¶”ì¶œ
        if 'localtime' in test_data.columns:
            test_times = test_data['localtime']
            
            # Submission ê¸°ê°„ê³¼ ê°€ì¥ ê°€ê¹Œìš´ ë¶€ë¶„ ì°¾ê¸°
            mask = (test_times >= start_time) & (test_times <= end_time)
            filtered_test = test_data[mask].copy()
            
            print(f"Submission ê¸°ê°„ê³¼ ë§¤ì¹­ëœ ë°ì´í„°: {len(filtered_test)}ê°œ")
            
            if len(filtered_test) < len(submission_template):
                print(f"ë§¤ì¹­ëœ ë°ì´í„° ë¶€ì¡±, ìµœì‹  {len(submission_template)}ê°œ ë°ì´í„° ì‚¬ìš©")
                filtered_test = test_data.tail(len(submission_template)).copy()
        else:
            print("localtime ì—†ìŒ, ìµœì‹  ë°ì´í„° ì‚¬ìš©")
            filtered_test = test_data.tail(len(submission_template)).copy()
        
        # 5. Submission í…œí”Œë¦¿ì— ë§ì¶° ì •ë ¬
        if len(filtered_test) > len(submission_template):
            filtered_test = filtered_test.head(len(submission_template))
        elif len(filtered_test) < len(submission_template):
            # ë¶€ì¡±í•œ ê²½ìš° ë§ˆì§€ë§‰ ê°’ìœ¼ë¡œ íŒ¨ë”©
            last_row = filtered_test.iloc[-1:].copy()
            needed = len(submission_template) - len(filtered_test)
            padding = pd.concat([last_row] * needed, ignore_index=True)
            filtered_test = pd.concat([filtered_test, padding], ignore_index=True)
        
        # Submission ID ì¶”ê°€
        filtered_test['submission_id'] = submission_template['id'].values
        
        # 6. í•µì‹¬ í”¼ì²˜ë§Œ ì„ íƒ ë° ëˆ„ë½ í”¼ì²˜ ì²˜ë¦¬ ê°œì„ 
        available_features = [col for col in self.feature_columns if col in filtered_test.columns]
        
        if len(available_features) != len(self.feature_columns):
            print(f"âš ï¸ ì¼ë¶€ í”¼ì²˜ê°€ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤:")
            missing_features = set(self.feature_columns) - set(available_features)
            print(f"   ëˆ„ë½ëœ í”¼ì²˜: {missing_features}")
            
            # ëˆ„ë½ëœ í”¼ì²˜ë¥¼ í›ˆë ¨ ë°ì´í„° í‰ê· ê°’ìœ¼ë¡œ ì±„ì›€ (0 ëŒ€ì‹ )
            for feature in missing_features:
                if feature in self.train_feature_stats:
                    fill_value = self.train_feature_stats[feature]['mean']
                    print(f"   {feature}: í›ˆë ¨ ë°ì´í„° í‰ê· ê°’ {fill_value:.4f}ë¡œ ì±„ì›€")
                else:
                    fill_value = 0
                    print(f"   âš ï¸ {feature}: í†µê³„ ì—†ìŒ, 0ìœ¼ë¡œ ì±„ì›€")
                filtered_test[feature] = fill_value
            
            available_features = self.feature_columns
        
        # 7. í”¼ì²˜ ë°ì´í„° ì¶”ì¶œ
        X_test = filtered_test[available_features].values
        
        # 8. ìŠ¤ì¼€ì¼ë§ ì ìš© (í›ˆë ¨ ë°ì´í„°ì™€ ë™ì¼í•œ ìŠ¤ì¼€ì¼ëŸ¬ ì‚¬ìš©)
        if apply_scaling and self.scaler is not None:
            print(f"í›ˆë ¨ ë°ì´í„°ì™€ ë™ì¼í•œ ìŠ¤ì¼€ì¼ë§ ì ìš© ì¤‘...")
            X_test_original = X_test.copy()
            X_test = self.scaler.transform(X_test)
            
            print(f"âœ… ìŠ¤ì¼€ì¼ë§ ì ìš© ì™„ë£Œ:")
            print(f"   - ìŠ¤ì¼€ì¼ë§ ì „: ë²”ìœ„ {X_test_original.min():.4f} ~ {X_test_original.max():.4f}")
            print(f"   - ìŠ¤ì¼€ì¼ë§ í›„: ë²”ìœ„ {X_test.min():.4f} ~ {X_test.max():.4f}")
        elif apply_scaling and self.scaler is None:
            print(f"âš ï¸ ìŠ¤ì¼€ì¼ëŸ¬ê°€ ì €ì¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ìŠ¤ì¼€ì¼ë§ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        
        print(f"âœ… í›ˆë ¨ ë°ì´í„°ì™€ ë™ì¼í•œ ì „ì²˜ë¦¬ ì™„ë£Œ:")
        print(f"   - X_test shape: {X_test.shape}")
        print(f"   - ì‚¬ìš©ëœ í”¼ì²˜: {len(available_features)}ê°œ")
        print(f"   - ì „ì²˜ë¦¬: ì™„ì „ (ê²°ì¸¡ì¹˜ + ì‹œê°„íŠ¹ì„± + ìŠ¤ì¼€ì¼ë§)")
        print(f"   - Submission í˜¸í™˜: âœ…")
        
        return X_test, filtered_test
    
    def save_ml_data(self, split_data: Dict, data_type: str = "tabular"):
        """ML í•™ìŠµìš© ë°ì´í„° ì €ì¥"""
        print(f"\nML í•™ìŠµìš© ë°ì´í„° ì €ì¥ ({data_type})")
        
        # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
        save_dir = project_root / "data" / "ml_ready" / data_type
        save_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # ë°ì´í„° ì €ì¥
        for split_name, data_array in split_data.items():
            if isinstance(data_array, np.ndarray):  # numpy ë°°ì—´ë§Œ ì €ì¥
                file_path = save_dir / f"{split_name}_{timestamp}.npy"
                np.save(file_path, data_array)
                print(f"   - {split_name}: {file_path}")
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥ (ìˆëŠ” ê²½ìš°)
        if self.scaler is not None:
            import pickle
            scaler_path = save_dir / f"scaler_{timestamp}.pkl"
            with open(scaler_path, 'wb') as f:
                pickle.dump(self.scaler, f)
            print(f"   - scaler: {scaler_path}")
        
        # í›ˆë ¨ ë°ì´í„° í†µê³„ ì €ì¥
        if self.train_feature_stats:
            stats_path = save_dir / f"train_stats_{timestamp}.json"
            with open(stats_path, 'w', encoding='utf-8') as f:
                json.dump(self.train_feature_stats, f, indent=2, ensure_ascii=False)
            print(f"   - train_stats: {stats_path}")
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            'data_type': data_type,
            'target_column': self.target_column,
            'feature_columns': self.feature_columns,
            'shapes': {name: data.shape for name, data in split_data.items() if isinstance(data, np.ndarray)},
            'timestamp': timestamp,
            'total_features': len(self.feature_columns),
            'has_scaler': self.scaler is not None,
            'has_train_stats': bool(self.train_feature_stats)
        }
        
        metadata_path = save_dir / f"metadata_{timestamp}.json"
        with open(metadata_path, 'w', encoding='utf-8') as f:
            json.dump(metadata, f, indent=2, ensure_ascii=False)
        
        print(f"ë©”íƒ€ë°ì´í„° ì €ì¥: {metadata_path}")
        
        return save_dir, timestamp
    
    def select_important_features(self, data: pd.DataFrame) -> List[str]:
        """ML/DLì— ì í•©í•œ í•µì‹¬ í”¼ì²˜ë§Œ ì„ íƒ"""
        print(f"\ní•µì‹¬ í”¼ì²˜ ì„ íƒ")
        
        # ìŠ¤ë§ˆíŠ¸ íŒ©í† ë¦¬ ì „ë ¥ ì˜ˆì¸¡ì— ì¤‘ìš”í•œ í”¼ì²˜ë“¤
        core_power_features = [
            'activePower',  # íƒ€ê²Ÿ (ì œì™¸ë  ì˜ˆì •)
            'voltageR', 'voltageS', 'voltageT',  # 3ìƒ ì „ì••
            'currentR', 'currentS', 'currentT',  # 3ìƒ ì „ë¥˜
            'powerFactorR', 'powerFactorS', 'powerFactorT',  # ì—­ë¥ 
            'accumActiveEnergy',  # ëˆ„ì  ì—ë„ˆì§€
            'operation'  # ìš´ì˜ ìƒíƒœ
        ]
        
        # ì‹œê°„ íŠ¹ì„± (í•µì‹¬ë§Œ)
        core_time_features = [
            'hour',  # ì‹œê°„
            'day_of_week',  # ìš”ì¼
            'month',  # ì›”
            'is_weekend',  # ì£¼ë§ ì—¬ë¶€
            'is_peak_hour',  # í”¼í¬ ì‹œê°„
            'is_business_hour'  # ì—…ë¬´ ì‹œê°„
        ]
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ í”¼ì²˜ë§Œ ì„ íƒ
        available_features = []
        
        for feature in core_power_features + core_time_features:
            if feature in data.columns and feature != self.target_column:
                available_features.append(feature)
        
        print(f"ì„ íƒëœ í•µì‹¬ í”¼ì²˜: {len(available_features)}ê°œ")
        print(f"   ì „ë ¥ ê´€ë ¨: {[f for f in available_features if any(k in f.lower() for k in ['voltage', 'current', 'power', 'energy', 'operation'])]}")
        print(f"   ì‹œê°„ ê´€ë ¨: {[f for f in available_features if f in core_time_features]}")
        
        return available_features
    
    def prepare_submission_template(self) -> pd.DataFrame:
        """submission í…œí”Œë¦¿ ì¤€ë¹„"""
        print(f"\nSubmission í…œí”Œë¦¿ ì¤€ë¹„")
        
        # sample_submission_final.csv ë¡œë“œ
        submission_path = project_root / "data" / "raw" / "sample_submission_final.csv"
        if not submission_path.exists():
            raise FileNotFoundError(f"Submission í…œí”Œë¦¿ì´ ì—†ìŠµë‹ˆë‹¤: {submission_path}")
        
        submission_template = pd.read_csv(submission_path)
        print(f"âœ… Submission í…œí”Œë¦¿ ë¡œë“œ: {submission_template.shape}")
        print(f"   ì»¬ëŸ¼: {list(submission_template.columns)}")
        print(f"   ì˜ˆì¸¡ ê¸°ê°„: {submission_template['id'].iloc[0]} ~ {submission_template['id'].iloc[-1]}")
        
        return submission_template

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ìŠ¤ë§ˆíŠ¸ íŒ©í† ë¦¬ ì „ë ¥ ì˜ˆì¸¡ - ML/DL í•™ìŠµ ë°ì´í„° ì¤€ë¹„")
    print("="*80)
    
    # ë°ì´í„° ì¤€ë¹„ê¸° ì´ˆê¸°í™”
    preparator = MLDataPreparator()
    
    try:
        # 1. ì „ì²˜ë¦¬ëœ Train ë°ì´í„° ë¡œë“œ (train.csv ê¸°ë°˜)
        train_data = preparator.load_preprocessed_data()
        
        # 2. íƒ€ê²Ÿê³¼ í”¼ì²˜ ì‹ë³„
        target_col, feature_cols = preparator.identify_target_and_features(train_data)
        
        # 2.5. í•µì‹¬ í”¼ì²˜ë§Œ ì„ íƒ (ë„ˆë¬´ ë§ì€ í”¼ì²˜ ë¬¸ì œ í•´ê²°)
        core_features = preparator.select_important_features(train_data)
        preparator.feature_columns = core_features  # ì„ íƒëœ í”¼ì²˜ë¡œ ì—…ë°ì´íŠ¸
        
        print(f"\nğŸ“Š í”¼ì²˜ ê°œìˆ˜ ë¹„êµ:")
        print(f"   ì „ì²´ í”¼ì²˜: {len(feature_cols)}ê°œ â†’ í•µì‹¬ í”¼ì²˜: {len(core_features)}ê°œ")
        
        # 2.6. Submission í…œí”Œë¦¿ ì¤€ë¹„
        submission_template = preparator.prepare_submission_template()
        
        # 3. ì‚¬ìš©ì ì„ íƒ
        print(f"\nğŸ”§ ML/DL ë°ì´í„° íƒ€ì…ì„ ì„ íƒí•˜ì„¸ìš”:")
        print(f"   1. í…Œì´ë¸” í˜•íƒœ (ì¼ë°˜ ML ëª¨ë¸ìš©)")
        print(f"   2. ì‹œê³„ì—´ ì‹œí€€ìŠ¤ (LSTM, GRU ë“± DL ëª¨ë¸ìš©)")
        print(f"   3. ë‘˜ ë‹¤ ìƒì„±")
        
        choice = input("\nì„ íƒ (1-3): ").strip()
        
        if choice == "1" or choice == "3":
            # í…Œì´ë¸” í˜•íƒœ ë°ì´í„° ìƒì„±
            print(f"\nğŸ“‹ í…Œì´ë¸” í˜•íƒœ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
            X_tab, y_tab = preparator.create_tabular_data(train_data)
            split_tab = preparator.split_data(X_tab, y_tab, time_series=False)
            
            # ìŠ¤ì¼€ì¼ë§ ì ìš© (í›ˆë ¨ ë°ì´í„° ê¸°ì¤€ìœ¼ë¡œ í”¼íŒ…)
            print(f"\nğŸ”§ ìŠ¤ì¼€ì¼ë§ ì ìš© ì¤‘...")
            from sklearn.preprocessing import StandardScaler
            preparator.scaler = StandardScaler()
            
            # í›ˆë ¨ ë°ì´í„°ë¡œ ìŠ¤ì¼€ì¼ëŸ¬ í•™ìŠµ
            X_train_original = split_tab['X_train'].copy()
            split_tab['X_train'] = preparator.scaler.fit_transform(split_tab['X_train'])
            split_tab['X_val'] = preparator.scaler.transform(split_tab['X_val'])
            
            print(f"âœ… ìŠ¤ì¼€ì¼ë§ ì™„ë£Œ:")
            print(f"   - ì›ë³¸ í›ˆë ¨ ë°ì´í„° ë²”ìœ„: {X_train_original.min():.4f} ~ {X_train_original.max():.4f}")
            print(f"   - ìŠ¤ì¼€ì¼ë§ í›„ ë²”ìœ„: {split_tab['X_train'].min():.4f} ~ {split_tab['X_train'].max():.4f}")
            
            # ì‹¤ì œ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ (ìŠ¤ì¼€ì¼ë§ ìë™ ì ìš©ë¨)
            X_test_tab, test_df = preparator.prepare_test_data(submission_template, apply_scaling=True)
            split_tab['X_test'] = X_test_tab
            split_tab['test_data'] = test_df  # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥ìš©
            
            save_dir_tab, timestamp_tab = preparator.save_ml_data(split_tab, "tabular")
        
        if choice == "2" or choice == "3":
            # ì‹œê³„ì—´ ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
            print(f"\nì‹œê³„ì—´ ì‹œí€€ìŠ¤ ë°ì´í„° ì¤€ë¹„ ì¤‘...")
            
            # ì‹œí€€ìŠ¤ ê¸¸ì´ ì„¤ì •
            print(f"\nì‹œê³„ì—´ ì„¤ì • ê¶Œì¥ì‚¬í•­:")
            print(f"   ìŠ¤ë§ˆíŠ¸ íŒ©í† ë¦¬ ì „ë ¥ ì˜ˆì¸¡ ìµœì í™”")
            print(f"   ì‹œí€€ìŠ¤ ê¸¸ì´ (Sequence Length):")
            print(f"      - 24ì‹œê°„: ì¼ì¼ íŒ¨í„´ í•™ìŠµ (ë¹ ë¦„, ê¸°ë³¸)")
            print(f"      - 48ì‹œê°„: 2ì¼ íŒ¨í„´ (ì•ˆì •ì„±)")  
            print(f"      - 72ì‹œê°„: 3ì¼ íŒ¨í„´ (ì£¼ë§ ê³ ë ¤)")
            print(f"   ğŸ¯ ì˜ˆì¸¡ êµ¬ê°„ (Prediction Horizon):")
            print(f"      - 1ì‹œê°„: 1-step ahead (ê¶Œì¥)")
            print(f"      - 6ì‹œê°„: 6ì‹œê°„ í›„ ì˜ˆì¸¡")
            print(f"      - 24ì‹œê°„: í•˜ë£¨ í›„ ì˜ˆì¸¡")
            
            print(f"\nğŸ’¡ ê¶Œì¥ ì¡°í•©:")
            print(f"   ğŸ¥‡ 1ìˆœìœ„: sequence=24, horizon=1 (ë¹ ë¥¸ í”„ë¡œí† íƒ€ì…)")
            print(f"   ğŸ¥ˆ 2ìˆœìœ„: sequence=48, horizon=1 (ì•ˆì •ì„±)")
            print(f"   ğŸ¥‰ 3ìˆœìœ„: sequence=72, horizon=1 (ì„±ëŠ¥ ìµœì í™”)")
            
            choice = input("\nì„¤ì •ì„ ì„ íƒí•˜ì„¸ìš” (1:ê¸°ë³¸, 2:ì•ˆì •ì„±, 3:ìµœì í™”, c:ì‚¬ìš©ìì •ì˜): ").strip().lower()
            
            if choice == "1":
                sequence_length, prediction_horizon = 24, 1
                print(f"ê¸°ë³¸ ì„¤ì • ì„ íƒ: {sequence_length}ì‹œê°„ â†’ {prediction_horizon}ì‹œê°„")
            elif choice == "2":
                sequence_length, prediction_horizon = 48, 1  
                print(f"ì•ˆì •ì„± ì„¤ì • ì„ íƒ: {sequence_length}ì‹œê°„ â†’ {prediction_horizon}ì‹œê°„")
            elif choice == "3":
                sequence_length, prediction_horizon = 72, 1
                print(f"ìµœì í™” ì„¤ì • ì„ íƒ: {sequence_length}ì‹œê°„ â†’ {prediction_horizon}ì‹œê°„")
            else:
                sequence_length = int(input("ì‹œí€€ìŠ¤ ê¸¸ì´ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸: 24): ") or "24")
                prediction_horizon = int(input("ì˜ˆì¸¡ êµ¬ê°„ì„ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸: 1): ") or "1")
            
            X_seq, y_seq = preparator.create_time_series_sequences(
                train_data, sequence_length, prediction_horizon
            )
            split_seq = preparator.split_data(X_seq, y_seq, time_series=True)
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë„ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜
            X_test_seq, test_df = preparator.prepare_test_data(submission_template)
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜í•˜ê¸° ìœ„í•´ ì„ì‹œë¡œ íƒ€ê²Ÿ ì»¬ëŸ¼ ì¶”ê°€
            test_df_copy = test_df.copy()
            test_df_copy[preparator.target_column] = 0  # ë”ë¯¸ ê°’
            
            # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ì‹œí€€ìŠ¤ë¡œ ë³€í™˜
            preparator_temp = MLDataPreparator()
            preparator_temp.feature_columns = preparator.feature_columns
            preparator_temp.target_column = preparator.target_column
            
            try:
                X_test_seq, _ = preparator_temp.create_time_series_sequences(
                    test_df_copy, sequence_length, prediction_horizon
                )
                split_seq['X_test'] = X_test_seq
                split_seq['test_data'] = test_df
            except Exception as e:
                print(f"í…ŒìŠ¤íŠ¸ ë°ì´í„° ì‹œí€€ìŠ¤ ë³€í™˜ ì‹¤íŒ¨: {e}")
                print(" í…ŒìŠ¤íŠ¸ ë°ì´í„°ëŠ” í…Œì´ë¸” í˜•íƒœë¡œë§Œ ì €ì¥ë©ë‹ˆë‹¤.")
                split_seq['X_test'] = X_test_seq
                split_seq['test_data'] = test_df
            
            save_dir_seq, timestamp_seq = preparator.save_ml_data(split_seq, "time_series")
        
        print(f"\nML/DL í•™ìŠµ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ")
        print(f"ê²°ê³¼ëŠ” 'data/ml_ready/' ë””ë ‰í† ë¦¬ì—ì„œ í™•ì¸.")
        
        # ì‚¬ìš©ë²• ì•ˆë‚´
        print(f"\n  ë°ì´í„° êµ¬ì¡°:")
        print(f"   - Train: í•™ìŠµìš© ë°ì´í„° (ë ˆì´ë¸” ìˆìŒ)")
        print(f"   - Validation: ê²€ì¦ìš© ë°ì´í„° (ë ˆì´ë¸” ìˆìŒ)")
        print(f"   - Test: ì‹¤ì œ ì˜ˆì¸¡ìš© ë°ì´í„° (ë ˆì´ë¸” ì—†ìŒ)")
        print(f"   - íƒ€ê²Ÿ ë³€ìˆ˜: {target_col}")
        print(f"   - í”¼ì²˜ ìˆ˜: {len(feature_cols)}ê°œ")
        
        print(f"\n  ì‚¬ìš©ë²•:")
        print(f"   - Pythonì—ì„œ: np.load('íŒŒì¼ê²½ë¡œ.npy')ë¡œ ë¡œë“œ")
        print(f"   - ë©”íƒ€ë°ì´í„°: JSON íŒŒì¼ì—ì„œ ì»¬ëŸ¼ ì •ë³´ í™•ì¸")
        print(f"   - ì˜ˆì¸¡ í›„ ê²°ê³¼ë¥¼ submission í˜•íƒœë¡œ ë³€í™˜ í•„ìš”")
        
    except Exception as e:
        print(f"ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 
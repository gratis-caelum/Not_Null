# ìŠ¤ë§ˆíŠ¸ íŒ©í† ë¦¬ ì „ë ¥ ì˜ˆì¸¡ AI ì±Œë¦°ì§€ ğŸ­âš¡

> **ì „ë ¥ ì†Œë¹„ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ í™œìš©í•œ ìŠ¤ë§ˆíŠ¸ íŒ©í† ë¦¬ ì „ë ¥ ì˜ˆì¸¡ ëª¨ë¸ ê°œë°œ**

## ğŸ“‹ í”„ë¡œì íŠ¸ ê°œìš”

ë³¸ í”„ë¡œì íŠ¸ëŠ” ê³µì¥ í˜„ì¥ì—ì„œ ë°œìƒí•˜ëŠ” ì„¤ë¹„ë³„ ì „ë ¥ ì†Œë¹„ ì‹œê³„ì—´ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ **2025ë…„ 5ì›” 1ì¼ 24ì‹œê°„ì˜ ì „ë ¥ ì†Œë¹„ëŸ‰ì„ ì˜ˆì¸¡**í•˜ëŠ” AI ëª¨ë¸ì„ ê°œë°œí•˜ëŠ” ê²ƒì´ ëª©í‘œì…ë‹ˆë‹¤.

### ğŸ¯ í‰ê°€ ê¸°ì¤€ (ì´ 100%)
- **ì¼ë³„ ì „ë ¥ ì˜ˆì¸¡** (30%): MAE, RMSE, SMAPE
- **ì£¼ë³„ ì „ë ¥ ì˜ˆì¸¡** (30%): MAE, RMSE, SMAPE  
- **5ì›” ì „ì²´ ì „ê¸°ìš”ê¸ˆ** (20%): ì´ ì „ë ¥ëŸ‰ Ã— 180ì›/kWh
- **5ì›” ì „ì²´ íƒ„ì†Œë°°ì¶œëŸ‰** (20%): ì´ ì „ë ¥ëŸ‰ Ã— 0.424kgCOâ‚‚/kWh

### ğŸ“Š ë°ì´í„° ì •ë³´
- **ì›ë³¸ ê·œëª¨**: 23,587,209 í–‰ Ã— 19 ì»¬ëŸ¼ (ì•½ 3.34GB)
- **ì²˜ë¦¬ëœ ìƒ˜í”Œ**: 100,000 í–‰ Ã— 97 ì»¬ëŸ¼ (ì•½ 107MB)
- **ê¸°ê°„**: 2024-12-01 ~ 2025-04-29 (ì‹œê°„ë³„ ì „ë ¥ ì†Œë¹„ ë°ì´í„°)
- **ì£¼ìš” ë³€ìˆ˜**: `activePower` (ì‹œê°„ë‹¹ ì „ë ¥), `accumActiveEnergy` (ëˆ„ì  ì „ë ¥)
- **ì„¤ë¹„ ì •ë³´**: `module(equipment)` - ë‹¤ì–‘í•œ ê³µì¥ ì„¤ë¹„
- **ì „ì•• ë°ì´í„°**: `voltageR`, `voltageS`, `voltageT`

## ğŸš€ ë¹ ë¥¸ ì‹œì‘ (Quick Start)

### 1. í™˜ê²½ ì„¤ì •
```bash
cd Not_Null/src/preprocessing
pip install pandas numpy scikit-learn matplotlib seaborn
```

### 2. ì „ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
```bash
python main.py
```

ì‹¤í–‰ í›„ ì„ íƒì‚¬í•­:
- **ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©**: `y` (ì¶”ì²œ, ë¹ ë¥¸ í…ŒìŠ¤íŠ¸ìš©)
- **ìƒ˜í”Œ í¬ê¸°**: `100000` (10ë§Œê°œ, ê¸°ë³¸ê°’)
- **ë¶„í•  ë°©ì‹**: `1` (submission ëª¨ë“œ, ì¶”ì²œ)
- **ë°ì´í„° ì €ì¥**: `y`

### 3. ì²˜ë¦¬ëœ ë°ì´í„° í™•ì¸
```
data/processed/
â”œâ”€â”€ train_data_20250523_115810.csv      # í›ˆë ¨ ë°ì´í„° (90,818ê°œ, 97MB)
â”œâ”€â”€ val_data_20250523_115810.csv        # ê²€ì¦ ë°ì´í„° (9,182ê°œ, 10MB)
â”œâ”€â”€ processed_features_20250523_115810.csv  # ì „ì²´ íŠ¹ì„± ë°ì´í„° (100,000ê°œ, 107MB)
â””â”€â”€ metadata_20250523_115810.json       # ë©”íƒ€ë°ì´í„° (1.3KB)
```

## ğŸ› ï¸ ëª¨ë“ˆ êµ¬ì¡°

```
src/preprocessing/
â”œâ”€â”€ config.py              # ì„¤ì • íŒŒì¼ (ìƒ˜í”Œ í¬ê¸°: 100,000)
â”œâ”€â”€ utils.py               # ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
â”œâ”€â”€ data_loader.py         # ë°ì´í„° ë¡œë”© (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
â”œâ”€â”€ feature_engineering.py # ì‹œê³„ì—´ íŠ¹ì„± ê³µí•™ (97ê°œ íŠ¹ì„± ìƒì„±)
â”œâ”€â”€ data_splitter.py       # ë°ì´í„° ë¶„í•  ë° í‰ê°€
â””â”€â”€ main.py               # í†µí•© íŒŒì´í”„ë¼ì¸
```

### ğŸ”§ ì£¼ìš” ê¸°ëŠ¥

#### 1. ë©”ëª¨ë¦¬ íš¨ìœ¨ì  ë°ì´í„° ë¡œë”©
```python
from data_loader import PowerDataLoader

loader = PowerDataLoader()
# ë¹ ë¥¸ ì •ë³´ í™•ì¸
info = loader.get_data_info()
# ì´ í–‰ ìˆ˜: 23,587,209, ì˜ˆìƒ ë©”ëª¨ë¦¬: 3.34GB

# 10ë§Œê°œ ìƒ˜í”Œ ë°ì´í„° ë¡œë”© (ìµœì í™”ëœ í¬ê¸°)
sample_data = loader.load_sample_data(100000)
# ë¡œë”© í›„: (100,000, 23) â†’ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ 50% ì ˆì•½

# ì²­í¬ ë°©ì‹ ì „ì²´ ë°ì´í„° ì²˜ë¦¬
for chunk in loader.load_chunks():
    # ì²­í¬ë³„ ì²˜ë¦¬
    pass
```

#### 2. ê³ ê¸‰ ì‹œê³„ì—´ íŠ¹ì„± ê³µí•™ (23ê°œ â†’ 97ê°œ íŠ¹ì„±)
```python
from feature_engineering import PowerFeatureEngineer

engineer = PowerFeatureEngineer()
featured_data = engineer.run_feature_engineering(data)

# ìƒì„±ë˜ëŠ” íŠ¹ì„±ë“¤ (ì´ 97ê°œ):
# - ê¸°ë³¸ íŠ¹ì„±: 23ê°œ
# - Lag features (8ê°œ): 1h, 2h, 3h, 6h, 12h, 24h, 48h, 168h
# - Rolling statistics (25ê°œ): 5ìœˆë„ìš° Ã— 5í†µê³„ëŸ‰ (mean, std, min, max, median)
# - Cyclical features (8ê°œ): hour_sin/cos, day_sin/cos, month_sin/cos, year_sin/cos
# - Difference features (8ê°œ): diff_1, diff_24, diff_168, pct_change ë“±
# - Voltage features (6ê°œ): mean, std, range, spike detection
# - Power ratios (6ê°œ): vs 24h mean, vs weekly mean, vs hourly mean
# - Anomaly detection (4ê°œ): z-score, IQR outliers, voltage anomaly
# - Equipment features (9ê°œ): ì„¤ë¹„ë³„ íŠ¹ì„± ì¶”ì¶œ
```

#### 3. ì‹œê³„ì—´ ë°ì´í„° ë¶„í• 
```python
from data_splitter import TimeSeriesSplitter

splitter = TimeSeriesSplitter()

# ì œì¶œìš© ë¶„í•  (ì¶”ì²œ) - 10ë§Œê°œ ìƒ˜í”Œ ê¸°ì¤€
train, val = splitter.create_submission_split(data)
# Train: 90,818ê°œ (2024-12-01 ~ 2025-04-15)
# Val: 9,182ê°œ (2025-04-15 ~ 2025-04-29)

# ë˜ëŠ” ë¹„ìœ¨ ê¸°ë°˜ ë¶„í• 
train, val, test = splitter.split_by_ratio(data)
```

#### 4. ì±Œë¦°ì§€ í‰ê°€ ì§€í‘œ
```python
from data_splitter import PowerPredictionEvaluator

evaluator = PowerPredictionEvaluator()
metrics = evaluator.comprehensive_evaluation(y_true, y_pred, datetime_index)

# ìë™ ê³„ì‚°ë˜ëŠ” ì§€í‘œ:
# - ì‹œê°„ë³„: MAE, RMSE, SMAPE
# - ì¼ë³„: MAE, RMSE, SMAPE (30% ê°€ì¤‘ì¹˜)
# - ì£¼ë³„: MAE, RMSE, SMAPE (30% ê°€ì¤‘ì¹˜)  
# - 5ì›” ì „ê¸°ìš”ê¸ˆ (20% ê°€ì¤‘ì¹˜)
# - 5ì›” íƒ„ì†Œë°°ì¶œëŸ‰ (20% ê°€ì¤‘ì¹˜)
```

## ğŸ¤– ML/DL ëª¨ë¸ ê°œë°œ ê°€ì´ë“œ

### ì¶”ì²œ ëª¨ë¸ (10ë§Œê°œ ìƒ˜í”Œ ìµœì í™”)
1. **LSTM/GRU**: ì‹œê³„ì—´ íŒ¨í„´ í•™ìŠµì— íš¨ê³¼ì , 97ê°œ íŠ¹ì„±ìœ¼ë¡œ í’ë¶€í•œ ì •ë³´
2. **XGBoost/LightGBM**: íŠ¹ì„± ê³µí•™ëœ ë°ì´í„°ì— ê°•ë ¥, ë¹ ë¥¸ í•™ìŠµ
3. **Transformer**: ì¥ê¸° ì˜ì¡´ì„± ëª¨ë¸ë§, ì¶©ë¶„í•œ ë°ì´í„° í¬ê¸°
4. **ARIMA/SARIMAX**: í†µê³„ì  ê¸°ì¤€ ëª¨ë¸

### ë°ì´í„° ë¡œë”© ì˜ˆì‹œ
```python
import pandas as pd

# ì²˜ë¦¬ëœ 10ë§Œê°œ ìƒ˜í”Œ ë°ì´í„° ë¡œë”©
train_data = pd.read_csv('data/processed/train_data_20250523_115810.csv', 
                        index_col=0, parse_dates=True)
val_data = pd.read_csv('data/processed/val_data_20250523_115810.csv', 
                      index_col=0, parse_dates=True)

print(f"í›ˆë ¨ ë°ì´í„°: {train_data.shape}")  # (90818, 97)
print(f"ê²€ì¦ ë°ì´í„°: {val_data.shape}")   # (9182, 97)
print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {train_data.memory_usage(deep=True).sum() / 1024**2:.1f}MB")

# íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
target_col = 'activePower'
feature_cols = [col for col in train_data.columns if col != target_col]

X_train = train_data[feature_cols]  # 96ê°œ íŠ¹ì„±
y_train = train_data[target_col]
X_val = val_data[feature_cols]
y_val = val_data[target_col]
```

### ëª¨ë¸ í•™ìŠµ ì˜ˆì‹œ (XGBoost)
```python
import xgboost as xgb
from data_splitter import PowerPredictionEvaluator

# 10ë§Œê°œ ìƒ˜í”Œì— ìµœì í™”ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„°
model = xgb.XGBRegressor(
    n_estimators=1000,
    max_depth=8,          # 97ê°œ íŠ¹ì„±ì— ë§ê²Œ ì¦ê°€
    learning_rate=0.1,
    subsample=0.8,
    colsample_bytree=0.8,
    random_state=42
)

model.fit(X_train, y_train,
          eval_set=[(X_val, y_val)],
          early_stopping_rounds=50,
          verbose=100)

# ì˜ˆì¸¡
y_pred = model.predict(X_val)

# í‰ê°€
evaluator = PowerPredictionEvaluator()
metrics = evaluator.comprehensive_evaluation(y_val, y_pred, val_data.index)
print(f"SMAPE: {metrics['smape']:.3f}")
print(f"MAE: {metrics['mae']:.3f}")
print(f"RMSE: {metrics['rmse']:.3f}")
```

### ì œì¶œ íŒŒì¼ ìƒì„±
```python
from data_splitter import create_submission_file

# 2025ë…„ 5ì›” 1ì¼ 24ì‹œê°„ ì˜ˆì¸¡
# (ì‹¤ì œë¡œëŠ” í•™ìŠµëœ ëª¨ë¸ë¡œ ì˜ˆì¸¡)
hourly_predictions = model.predict(may_features)  # 24ê°œ ê°’
agg_predictions = np.cumsum(hourly_predictions)   # ëˆ„ì  ê°’
may_bill = np.sum(hourly_predictions) * 180      # 5ì›” ì „ì²´ ì „ê¸°ìš”ê¸ˆ
may_carbon = np.sum(hourly_predictions) * 0.424  # 5ì›” ì „ì²´ íƒ„ì†Œë°°ì¶œëŸ‰

submission = create_submission_file(
    hourly_predictions, agg_predictions,
    may_bill, may_carbon,
    'submission.csv'
)
```

## ğŸ“ˆ íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ (97ê°œ íŠ¹ì„±)

ìƒì„±ëœ íŠ¹ì„±ë“¤ì€ ë‹¤ìŒê³¼ ê°™ì´ ê·¸ë£¹í™”ë©ë‹ˆë‹¤:

```python
feature_groups = {
    'lag_features': [8ê°œ],      # ì§€ì—° íŠ¹ì„± (1h~168h)
    'rolling_stats': [25ê°œ],    # ì´ë™ í†µê³„ (5ìœˆë„ìš°Ã—5í†µê³„ëŸ‰)
    'cyclical': [8ê°œ],          # ì‚¬ì´í´ë¦­ ì‹œê°„ íŠ¹ì„±
    'difference': [8ê°œ],        # ì°¨ë¶„ íŠ¹ì„±
    'voltage': [6ê°œ],           # ì „ì•• ê´€ë ¨ íŠ¹ì„±
    'ratios': [6ê°œ],           # ì „ë ¥ ë¹„ìœ¨ íŠ¹ì„±
    'anomaly': [4ê°œ],          # ì´ìƒì¹˜ íƒì§€ íŠ¹ì„±
    'equipment': [9ê°œ],        # ì„¤ë¹„ë³„ íŠ¹ì„±
    'original': [23ê°œ]         # ì›ë³¸ íŠ¹ì„±
}
```

### íŠ¹ì„± ì¤‘ìš”ë„ ì˜ˆì‹œ ë¶„ì„
```python
import matplotlib.pyplot as plt

# XGBoost íŠ¹ì„± ì¤‘ìš”ë„
importance = model.feature_importances_
feature_importance = pd.DataFrame({
    'feature': feature_cols,
    'importance': importance
}).sort_values('importance', ascending=False)

# Top 20 íŠ¹ì„± ì‹œê°í™”
plt.figure(figsize=(12, 8))
top_features = feature_importance.head(20)
plt.barh(range(len(top_features)), top_features['importance'])
plt.yticks(range(len(top_features)), top_features['feature'])
plt.title('Top 20 íŠ¹ì„± ì¤‘ìš”ë„ (10ë§Œê°œ ìƒ˜í”Œ ê¸°ì¤€)')
plt.xlabel('ì¤‘ìš”ë„')
plt.tight_layout()
plt.show()
```

## ğŸ” ì„±ëŠ¥ ìµœì í™” íŒ (10ë§Œê°œ ìƒ˜í”Œ)

### 1. ë©”ëª¨ë¦¬ ìµœì í™” (ì´ë¯¸ ì ìš©ë¨)
```python
# ë°ì´í„° íƒ€ì… ìµœì í™”ë¡œ ë©”ëª¨ë¦¬ 50% ì ˆì•½
# float64 â†’ float32: ë©”ëª¨ë¦¬ 50% ì ˆì•½
# int64 â†’ int32: ë©”ëª¨ë¦¬ 50% ì ˆì•½
# object â†’ category: ë©”ëª¨ë¦¬ ëŒ€í­ ì ˆì•½
# 10ë§Œê°œ Ã— 97ê°œ íŠ¹ì„± = ì•½ 56MB ë©”ëª¨ë¦¬ ì‚¬ìš©
```

### 2. ì‹œê³„ì—´ êµì°¨ ê²€ì¦ (10ë§Œê°œ ìƒ˜í”Œ ìµœì í™”)
```python
from sklearn.model_selection import TimeSeriesSplit

# 10ë§Œê°œ ìƒ˜í”Œì— ì í•©í•œ ë¶„í•  ìˆ˜
tscv = TimeSeriesSplit(n_splits=5)  # ê° fold: ~18,000ê°œ ìƒ˜í”Œ
for fold, (train_idx, val_idx) in enumerate(tscv.split(X_train)):
    print(f"Fold {fold+1}: Train {len(train_idx)}, Val {len(val_idx)}")
    # ëª¨ë¸ í•™ìŠµ ë° ê²€ì¦
```

### 3. í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (10ë§Œê°œ ìµœì í™”)
```python
import optuna

def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 500, 2000),
        'max_depth': trial.suggest_int('max_depth', 6, 12),        # 97ê°œ íŠ¹ì„± ê³ ë ¤
        'learning_rate': trial.suggest_float('learning_rate', 0.05, 0.3),
        'subsample': trial.suggest_float('subsample', 0.7, 0.9),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.7, 0.9),
    }
    
    model = xgb.XGBRegressor(**params, random_state=42)
    model.fit(X_train, y_train, 
              eval_set=[(X_val, y_val)], 
              early_stopping_rounds=50, 
              verbose=0)
    
    y_pred = model.predict(X_val)
    evaluator = PowerPredictionEvaluator()
    smape = evaluator.smape(y_val, y_pred)
    return smape

study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=100)
print(f"Best SMAPE: {study.best_value:.4f}")
print(f"Best params: {study.best_params}")
```

### 4. ì•™ìƒë¸” ëª¨ë¸ (10ë§Œê°œ ìƒ˜í”Œ)
```python
from sklearn.ensemble import VotingRegressor
from sklearn.linear_model import Ridge
from catboost import CatBoostRegressor

# 10ë§Œê°œ ìƒ˜í”Œë¡œ ì¶©ë¶„í•œ ì•™ìƒë¸” í•™ìŠµ
models = [
    ('xgb', xgb.XGBRegressor(n_estimators=1000, max_depth=8)),
    ('cat', CatBoostRegressor(iterations=1000, depth=8, verbose=False)),
    ('ridge', Ridge(alpha=1.0))
]

ensemble = VotingRegressor(models)
ensemble.fit(X_train, y_train)

# ì•™ìƒë¸” ì˜ˆì¸¡
y_pred_ensemble = ensemble.predict(X_val)
```

## â— ì£¼ì˜ì‚¬í•­ (10ë§Œê°œ ìƒ˜í”Œ íŠ¹í™”)

### 1. ì‹œê°„ ìˆœì„œ ì¤€ìˆ˜
- **ì ˆëŒ€** ë¯¸ë˜ ë°ì´í„°ê°€ ê³¼ê±° ì˜ˆì¸¡ì— ì‚¬ìš©ë˜ì§€ ì•Šë„ë¡ ì£¼ì˜
- íŠ¹ì„± ìƒì„± ì‹œ lagì™€ rolling window ê³ ë ¤ (ìµœëŒ€ 168h)
- ë°ì´í„° ë¶„í• : Train (90,818ê°œ) â†’ Val (9,182ê°œ)

### 2. ë©”ëª¨ë¦¬ ê´€ë¦¬ (ìµœì í™”ë¨)
- 10ë§Œê°œ ìƒ˜í”Œ: ì•½ 107MB (ê´€ë¦¬ ê°€ëŠ¥í•œ í¬ê¸°)
- 97ê°œ íŠ¹ì„±: ë©”ëª¨ë¦¬ íš¨ìœ¨ì ìœ¼ë¡œ ìµœì í™”ë¨
- ì „ì²´ 23M ë°ì´í„°ëŠ” í•„ìš”ì‹œì—ë§Œ ì‚¬ìš©

### 3. íƒ€ê²Ÿ ë³€ìˆ˜
- `activePower`: ì‹œê°„ë‹¹ ì „ë ¥ ì†Œë¹„ëŸ‰ (kW) - **ì£¼ìš” íƒ€ê²Ÿ**
- ë²”ìœ„: 890~5,190 kW, í‰ê· : 3,011 kW
- `accumActiveEnergy`: ëˆ„ì  ì „ë ¥ ì†Œë¹„ëŸ‰ (kWh)

### 4. ê²°ì¸¡ì¹˜ ë° ì´ìƒì¹˜ (ì²˜ë¦¬ë¨)
- ì „ì²˜ë¦¬ ë‹¨ê³„ì—ì„œ 2,397ê°œ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ë¨
- ì´ìƒì¹˜ íƒì§€ íŠ¹ì„± í¬í•¨ (z-score, IQR)
- ìƒˆë¡œìš´ ê²°ì¸¡ì¹˜ ë°œìƒ ì‹œ forward fill â†’ backward fill ì ìš©

## ğŸ† ìµœì¢… ì²´í¬ë¦¬ìŠ¤íŠ¸ (10ë§Œê°œ ìƒ˜í”Œ)

### ëª¨ë¸ ê°œë°œ ì „
- [ ] 10ë§Œê°œ ìƒ˜í”Œ ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ì™„ë£Œ âœ…
- [ ] 97ê°œ íŠ¹ì„± ê³µí•™ ê²°ê³¼ í™•ì¸ âœ…
- [ ] í›ˆë ¨(90,818)/ê²€ì¦(9,182) ë°ì´í„° ë¶„í•  í™•ì¸ âœ…
- [ ] íƒ€ê²Ÿ ë³€ìˆ˜ ë¶„í¬ ë¶„ì„ (í‰ê· : 3,011 kW)

### ëª¨ë¸ í•™ìŠµ ì¤‘
- [ ] ì‹œê³„ì—´ êµì°¨ ê²€ì¦ ì ìš© (5-fold ê¶Œì¥)
- [ ] í‰ê°€ ì§€í‘œ ëª¨ë‹ˆí„°ë§ (MAE, RMSE, SMAPE)
- [ ] ê³¼ì í•© ë°©ì§€ (early stopping, 97ê°œ íŠ¹ì„± ê³ ë ¤)
- [ ] íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„ (ê·¸ë£¹ë³„ ê¸°ì—¬ë„)

### ì œì¶œ ì „
- [ ] 2025-05-01 24ì‹œê°„ ì˜ˆì¸¡ ìƒì„±
- [ ] submission íŒŒì¼ í˜•ì‹ í™•ì¸
- [ ] ì˜ˆì¸¡ê°’ ë²”ìœ„ ê²€ì¦ (890~5,190 kW ë‚´)
- [ ] 5ì›” ì „ê¸°ìš”ê¸ˆ, íƒ„ì†Œë°°ì¶œëŸ‰ ê³„ì‚° í™•ì¸

## ğŸ“Š ë°ì´í„° í†µê³„ (10ë§Œê°œ ìƒ˜í”Œ)

```
ë°ì´í„° ê°œìš”:
- ì´ ìƒ˜í”Œ: 100,000ê°œ
- ì´ íŠ¹ì„±: 97ê°œ (23ê°œ â†’ 97ê°œë¡œ í™•ì¥)
- ì‹œê°„ ë²”ìœ„: 2024-12-01 ~ 2025-04-29
- ì£¼íŒŒìˆ˜: ì‹œê°„ë³„ (hourly)
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: 56.3MB

í›ˆë ¨/ê²€ì¦ ë¶„í• :
- í›ˆë ¨ ë°ì´í„°: 90,818ê°œ (90.8%)
- ê²€ì¦ ë°ì´í„°: 9,182ê°œ (9.2%)
- ì˜ˆì¸¡ ì‹œì‘: 2025-05-01 00:00:00

íƒ€ê²Ÿ ë³€ìˆ˜ í†µê³„:
- í‰ê· : 3,011 kW
- í‘œì¤€í¸ì°¨: 789 kW
- ìµœì†Œê°’: 890 kW
- ìµœëŒ€ê°’: 5,190 kW
- ì¤‘ì•™ê°’: 3,045 kW
```

## ğŸ“ ë¬¸ì˜ ë° ì§€ì›

- **ì´ìŠˆ íŠ¸ë˜í‚¹**: GitHub Issues
- **ì½”ë“œ ë¦¬ë·°**: Pull Request
- **íŒ€ íšŒì˜**: ì£¼ê°„ ì§„í–‰ ìƒí™© ê³µìœ 

---

## ğŸ“ íŒŒì¼ êµ¬ì¡°

```
Not_Null/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                    # ì›ë³¸ ë°ì´í„°
â”‚   â”‚   â”œâ”€â”€ train.csv          # í›ˆë ¨ ë°ì´í„° (23,587,209 í–‰, 3.34GB)
â”‚   â”‚   â”œâ”€â”€ test.csv           # í…ŒìŠ¤íŠ¸ ë°ì´í„°
â”‚   â”‚   â””â”€â”€ sample_submission_final.csv
â”‚   â””â”€â”€ processed/              # ì „ì²˜ë¦¬ëœ ë°ì´í„° (10ë§Œê°œ ìƒ˜í”Œ)
â”‚       â”œâ”€â”€ train_data_20250523_115810.csv      # 90,818ê°œ, 97MB
â”‚       â”œâ”€â”€ val_data_20250523_115810.csv        # 9,182ê°œ, 10MB  
â”‚       â”œâ”€â”€ processed_features_20250523_115810.csv # 100,000ê°œ, 107MB
â”‚       â””â”€â”€ metadata_20250523_115810.json       # ë©”íƒ€ë°ì´í„°, 1.3KB
â”œâ”€â”€ src/
â”‚   â””â”€â”€ preprocessing/          # ì „ì²˜ë¦¬ ëª¨ë“ˆ (97ê°œ íŠ¹ì„± ìƒì„±)
â”œâ”€â”€ notebooks/                  # EDA ë…¸íŠ¸ë¶
â”œâ”€â”€ reports/                    # ê²°ê³¼ ë³´ê³ ì„œ
â””â”€â”€ README.md                   # ì´ íŒŒì¼
```

**10ë§Œê°œ ìƒ˜í”Œë¡œ ì„±ê³µì ì¸ ëª¨ë¸ ê°œë°œì„ ì‘ì›í•©ë‹ˆë‹¤! ğŸ€**